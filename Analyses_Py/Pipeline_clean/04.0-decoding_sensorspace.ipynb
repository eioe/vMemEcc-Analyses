{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be0b531a",
   "metadata": {},
   "source": [
    "# Decoding memory load from the ERP\n",
    "\n",
    "In this notebook, we train a sliding classifier to decode the memory load from the 60-channel EEG data. \n",
    "\n",
    "2022 -- Felix Klotzsche "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca4e4113",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# %% load libs:\n",
    "from collections import defaultdict\n",
    "from os import path as op\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import mne\n",
    "# from mne.epochs import concatenate_epochs\n",
    "from mne.decoding import (SlidingEstimator, GeneralizingEstimator,\n",
    "                          cross_val_multiscore, LinearModel, get_coef)\n",
    "\n",
    "from library import config, helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a79f12f",
   "metadata": {
    "lines_to_next_cell": 2,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4a53993",
   "metadata": {
    "lines_to_next_cell": 0,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# %% Functions:\n",
    "\n",
    "def get_epos(subID, epo_part, signaltype, condition, event_dict, picks_str):\n",
    "    if signaltype == 'uncollapsed':\n",
    "        fname = op.join(config.paths['03_preproc-rejectET'],\n",
    "                        epo_part,\n",
    "                        'cleaneddata',\n",
    "                        f\"{subID}-{epo_part}-rejepo-epo.fif\")\n",
    "    elif signaltype in ['collapsed', 'difference']:\n",
    "        fname = op.join(config.paths['03_preproc-pooled'],\n",
    "                        epo_part,\n",
    "                        signaltype,\n",
    "                        f\"{subID}-{epo_part}-{signaltype}-epo.fif\")\n",
    "    else:\n",
    "        raise ValueError(f'Invalid value for \"signaltype\": {signaltype}')\n",
    "    epos = mne.read_epochs(fname, verbose=False)\n",
    "    epos = epos.pick_types(eeg=True)\n",
    "\n",
    "    # pick channel selection:\n",
    "    if (picks_str is not None) and (picks_str != 'All'):\n",
    "        roi_dict = mne.channels.make_1020_channel_selections(epos.info)\n",
    "        picks = [epos.ch_names[idx] for idx in roi_dict[picks_str]]\n",
    "        epos.pick_channels(picks, ordered=True)\n",
    "\n",
    "    uppers = [letter.isupper() for letter in condition]\n",
    "    if (np.sum(uppers) > 2):\n",
    "        cond_1 = condition[:np.where(uppers)[0][2]]\n",
    "        cond_2 = condition[np.where(uppers)[0][2]:]\n",
    "        selection = epos[event_dict[cond_1]][event_dict[cond_2]]\n",
    "    else:\n",
    "        selection = epos[event_dict[condition]]\n",
    "    return(selection)\n",
    "\n",
    "\n",
    "def avg_time(data, step=25, times=None):\n",
    "    orig_shape = data.shape\n",
    "    n_fill = step - (orig_shape[-1] % step)\n",
    "    fill_shape = np.asarray(orig_shape)\n",
    "    fill_shape[-1] = n_fill\n",
    "    fill = np.ones(fill_shape) * np.nan\n",
    "    data_f = np.concatenate([data, fill], axis=-1)\n",
    "    data_res = np.nanmean(data_f.reshape(*orig_shape[:2], -1, step), axis=-1)\n",
    "\n",
    "    if times is not None:\n",
    "        f_times = np.r_[times, [np.nan] * n_fill]\n",
    "        n_times = np.nanmean(f_times.reshape(-1, step), axis=-1)\n",
    "        return data_res, n_times\n",
    "    else:\n",
    "        return data_res\n",
    "\n",
    "\n",
    "def batch_trials(epos, batch_size):\n",
    "    n_trials = len(epos)\n",
    "    n_batches = int(n_trials / batch_size)\n",
    "    rnd_seq = np.arange(n_trials)\n",
    "    np.random.shuffle(rnd_seq)\n",
    "    rnd_seq = rnd_seq[:n_batches * batch_size]\n",
    "    rnd_seq = rnd_seq.reshape(-1, batch_size)\n",
    "    batches = [epos[b].average() for b in rnd_seq]\n",
    "    return(batches)\n",
    "\n",
    "\n",
    "def get_data(subID, epo_part, signaltype, conditions, event_dict,\n",
    "             batch_size=1, smooth_winsize=1, picks_str=None):\n",
    "    epos_dict = defaultdict(dict)\n",
    "    for cond in conditions:\n",
    "        epos_dict[cond] = get_epos(subID,\n",
    "                                   epo_part=epo_part,\n",
    "                                   signaltype=signaltype,\n",
    "                                   condition=cond,\n",
    "                                   event_dict=event_dict,\n",
    "                                   picks_str=picks_str)\n",
    "\n",
    "    times = epos_dict[conditions[0]][0].copy().times\n",
    "    info = epos_dict[conditions[0]][0].info\n",
    "\n",
    "    # Setup data:\n",
    "    if batch_size > 1:\n",
    "        batches = defaultdict(list)\n",
    "        for cond in conditions:\n",
    "            batches[cond] = batch_trials(epos_dict[cond], batch_size)\n",
    "            batches[cond] = np.asarray([b.data for b in batches[cond]])\n",
    "\n",
    "        X = np.concatenate([batches[cond].data for cond in conditions], axis=0)\n",
    "        n_ = {cond: batches[cond].shape[0] for cond in conditions}\n",
    "\n",
    "    else:\n",
    "        X = mne.concatenate_epochs([epos_dict[cond] for cond in conditions])\n",
    "        X = X.get_data()\n",
    "        n_ = {cond: len(epos_dict[cond]) for cond in conditions}\n",
    "\n",
    "    if smooth_winsize > 1:\n",
    "        X, times_n = avg_time(X, smooth_winsize, times=times)\n",
    "    else:\n",
    "        times_n = times\n",
    "\n",
    "    y = np.r_[np.zeros(n_[conditions[0]]),\n",
    "              np.concatenate([(np.ones(n_[conditions[i]]) * i)\n",
    "                              for i in np.arange(1, len(conditions))])]\n",
    "\n",
    "    return X, y, times_n, info\n",
    "\n",
    "\n",
    "def decode(sub_list_str,\n",
    "           conditions,\n",
    "           epo_part='stimon',\n",
    "           signaltype='collapsed',\n",
    "           scoring='roc_auc',\n",
    "           event_dict=config.event_dict,\n",
    "           n_rep_sub=100,\n",
    "           picks_str=None,\n",
    "           shuffle_labels=False,\n",
    "           batch_size=10,\n",
    "           smooth_winsize=5,\n",
    "           n_cv_folds=5,\n",
    "           temp_gen=False,\n",
    "           save_single_rep_scores=False,\n",
    "           save_scores=True,\n",
    "           save_patterns=False):\n",
    "\n",
    "    contrast_str = '_vs_'.join(conditions)\n",
    "    scoring = scoring  # 'roc_auc' # 'accuracy'\n",
    "    cv_folds = n_cv_folds\n",
    "\n",
    "    subs_processed = list()\n",
    "    sub_scores = list()\n",
    "    sub_scores_per_rep = list()\n",
    "    sub_coef = list()\n",
    "    times_n = list()\n",
    "\n",
    "    for subID in sub_list_str:\n",
    "        print(f'### RUNING SUBJECT {subID}')\n",
    "        subs_processed.append(subID)\n",
    "        all_scores = list()\n",
    "        all_coef = list()\n",
    "        for i in np.arange(n_rep_sub):\n",
    "            X, y, times_n, info = get_data(subID,\n",
    "                                           epo_part=epo_part,\n",
    "                                           signaltype=signaltype,\n",
    "                                           conditions=conditions,\n",
    "                                           event_dict=event_dict,\n",
    "                                           batch_size=batch_size,\n",
    "                                           smooth_winsize=smooth_winsize,\n",
    "                                           picks_str=picks_str)\n",
    "\n",
    "            clf = make_pipeline(mne.decoding.Scaler(info),\n",
    "                                mne.decoding.Vectorizer(),\n",
    "                                LinearModel(\n",
    "                                    LogisticRegression(solver='liblinear',\n",
    "                                                       random_state=42,\n",
    "                                                       verbose=False)))\n",
    "\n",
    "            # TODO: refactor: rename \"se\"\n",
    "            if temp_gen:\n",
    "                gen_str = 'gen_temp'\n",
    "                se = GeneralizingEstimator(clf,\n",
    "                                           scoring=scoring,\n",
    "                                           n_jobs=-2,\n",
    "                                           verbose=0)\n",
    "            else:\n",
    "                gen_str = ''\n",
    "                se = SlidingEstimator(clf,\n",
    "                                      scoring=scoring,\n",
    "                                      n_jobs=-2,\n",
    "                                      verbose=0)\n",
    "\n",
    "            if shuffle_labels:\n",
    "                np.random.shuffle(y)\n",
    "            for i in np.unique(y):\n",
    "                print(f'Size of class {i}: {np.sum(y == i)}\\n')\n",
    "            scores = cross_val_multiscore(se, X=X, y=y, cv=cv_folds, verbose=0)\n",
    "            scores = np.mean(scores, axis=0)\n",
    "            all_scores.append(scores)\n",
    "            se.fit(X, y)\n",
    "            coef = get_coef(se, 'patterns_', inverse_transform=True)\n",
    "            all_coef.append(coef)\n",
    "\n",
    "        sub_scores = np.asarray(all_scores).mean(axis=0)\n",
    "        sub_coef = np.asarray(all_coef).mean(axis=0)\n",
    "\n",
    "        # save shizzle:\n",
    "        shuf_labs = 'labels_shuffled' if shuffle_labels else ''\n",
    "\n",
    "        if picks_str is not None:\n",
    "            picks_str_folder = picks_str\n",
    "        else:\n",
    "            picks_str_folder = ''\n",
    "\n",
    "        path_save = op.join(config.paths['06_decoding-sensorspace'], epo_part,\n",
    "                            signaltype, contrast_str, gen_str,\n",
    "                            scoring, picks_str_folder, shuf_labs)\n",
    "\n",
    "        # save accuracies:\n",
    "        if save_scores:\n",
    "            fpath = op.join(path_save, 'scores')\n",
    "            helpers.chkmk_dir(fpath)\n",
    "            fname = op.join(fpath, f'{subID}-scores_per_sub.npy')\n",
    "            np.save(fname, sub_scores)\n",
    "            np.save(fname[:-4] + '__times' + '.npy', times_n)\n",
    "            del(fpath, fname)\n",
    "\n",
    "        # save patterns:\n",
    "        if save_patterns:\n",
    "            sub_patterns = sub_coef\n",
    "            fpath = op.join(path_save, 'patterns')\n",
    "            helpers.chkmk_dir(fpath)\n",
    "            fname = op.join(fpath, f'{subID}-patterns_per_sub.npy')\n",
    "            np.save(fname, sub_patterns)\n",
    "            np.save(fname[:-4] + '__times' + '.npy', times_n)\n",
    "            del(fpath, fname)\n",
    "\n",
    "        # save info:\n",
    "        if save_scores or save_patterns or save_single_rep_scores:\n",
    "            info_dict = {'included subs': subs_processed,\n",
    "                         'n_rep_sub': n_rep_sub,\n",
    "                         'batch_size': batch_size,\n",
    "                         'smooth_winsize': smooth_winsize,\n",
    "                         'cv_folds': cv_folds,\n",
    "                         'scoring': scoring}\n",
    "            fpath = path_save\n",
    "            fname = op.join(fpath, f'{subID}-info.json')\n",
    "            with open(fname, 'w+') as outfile:\n",
    "                json.dump(info_dict, outfile)\n",
    "\n",
    "        # save data from single reps:\n",
    "        if save_single_rep_scores:\n",
    "            if len(sub_scores_per_rep) == 0:\n",
    "                sub_scores_per_rep = np.asarray(all_scores)\n",
    "            else:\n",
    "                sub_scores_per_rep = np.concatenate([sub_scores_per_rep,\n",
    "                                                    np.asarray(all_scores)],\n",
    "                                                    axis=0)\n",
    "\n",
    "            fpath = op.join(path_save, 'single_rep_data')\n",
    "            helpers.chkmk_dir(fpath)\n",
    "            fname = op.join(fpath,\n",
    "                            f'{subID}-'\n",
    "                            f'reps{n_rep_sub}_'\n",
    "                            f'swin{smooth_winsize}_batchs{batch_size}.npy')\n",
    "            np.save(fname, sub_scores_per_rep)\n",
    "            np.save(fname[:-4] + '__times' + '.npy', times_n)\n",
    "            del(fpath, fname)\n",
    "\n",
    "    return sub_scores, sub_coef, times_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5168d7d4",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Overwriting old value of decod_sensorspace_cv_folds_train_perc (20) with: 80\n"
     ]
    }
   ],
   "source": [
    "# %% setup params:\n",
    "\n",
    "mne.set_log_level('WARNING')\n",
    "\n",
    "# plotting:\n",
    "plt_dict = defaultdict(dict)\n",
    "pp = {'t_stimon':  0,\n",
    "      'xmin': -0.2,\n",
    "      'xmax': 2.3}\n",
    "plt_dict['stimon'] = pp\n",
    "\n",
    "# Set up parameters:\n",
    "batch_size = 10\n",
    "smooth_winsize = 10\n",
    "n_rep_sub = 100\n",
    "n_cv_folds = 5\n",
    "\n",
    "# Extract params:\n",
    "helpers.extract_var(\"decod_sensorspace_batch_size\", batch_size, exp_format=\".0f\")\n",
    "helpers.extract_var(\"decod_sensorspace_smooth_winsize\", smooth_winsize, exp_format=\".0f\")\n",
    "helpers.extract_var(\"decod_sensorspace_cv_folds\", n_cv_folds, exp_format=\".0f\")\n",
    "helpers.extract_var(\"decod_sensorspace_n_rep_sub\", n_rep_sub, exp_format=\".0f\")\n",
    "if ((100 * 1 / n_cv_folds) % 1) > 0.0:\n",
    "      exp_f_perc = \".2f\"\n",
    "else:\n",
    "      exp_f_perc = \".0f\"\n",
    "helpers.extract_var(\"decod_sensorspace_cv_folds_train_perc\", 100 * (n_cv_folds - 1) / n_cv_folds,\n",
    "                    exp_format=exp_f_perc)\n",
    "helpers.extract_var(\"decod_sensorspace_cv_folds_test_perc\", 100 * 1 / n_cv_folds,\n",
    "                    exp_format=exp_f_perc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5b0728",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# structuring data:\n",
    "sub_list = np.setdiff1d(np.arange(1, 28), config.ids_missing_subjects +\n",
    "                        config.ids_excluded_subjects)               \n",
    "sub_list_str = ['VME_S%02d' % sub for sub in sub_list]\n",
    "\n",
    "# when running on the cluster we want parallelization along the subject dimension\n",
    "if not helpers.is_interactive(): \n",
    "    helpers.print_msg('Running Job Nr. ' + sys.argv[1])\n",
    "    job_nr = int(float(sys.argv[1]))\n",
    "    sub_list_str = [sub_list_str[job_nr]]   \n",
    "\n",
    "event_dict = config.event_dict\n",
    "cond_dict = {'Load': ['LoadLow', 'LoadHigh'],\n",
    "             'Ecc': ['EccS', 'EccM', 'EccL']}\n",
    "\n",
    "\n",
    "\n",
    "# %% Decode load across all eccentricities:\n",
    "\n",
    "decod_results_load = defaultdict(dict)\n",
    "\n",
    "\n",
    "# for shuf_labs in [False, True]:\n",
    "#     for picks_str in ['All']: # ['Right', 'Left']: \n",
    "#         conditions = ['LoadLow', 'LoadHigh']\n",
    "#         contrast_str = '_vs_'.join(conditions)\n",
    "#         sc_, pat_, ts_ = decode(sub_list_str, \n",
    "#                                 conditions=conditions,\n",
    "#                                 epo_part='stimon', \n",
    "#                                 signaltype='collapsed',\n",
    "#                                 event_dict=config.event_dict, \n",
    "#                                 n_rep_sub=n_rep_sub,\n",
    "#                                 picks_str=picks_str,\n",
    "#                                 shuffle_labels=shuf_labs,\n",
    "#                                 batch_size=batch_size,\n",
    "#                                 n_cv_folds=n_cv_folds,\n",
    "#                                 temp_gen=False,\n",
    "#                                 smooth_winsize=smooth_winsize,\n",
    "#                                 save_single_rep_scores=True,\n",
    "#                                 save_patterns=True,\n",
    "#                                 save_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06aae5a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# %% decode load per eccentricity:\n",
    "\n",
    "decod_results_load = defaultdict(dict)\n",
    "\n",
    "for ecc in ['EccL']:  # cond_dict['Ecc']:\n",
    "    conditions = ['LoadLow' + ecc, 'LoadHigh' + ecc]\n",
    "    contrast_str = '_vs_'.join(conditions)\n",
    "    for shuf_labs in [False, True]:\n",
    "        sc_, pat_, ts_ = decode(sub_list_str,\n",
    "                                conditions=conditions,\n",
    "                                epo_part='stimon',\n",
    "                                signaltype='collapsed',\n",
    "                                scoring='roc_auc',\n",
    "                                event_dict=config.event_dict,\n",
    "                                n_rep_sub=n_rep_sub,\n",
    "                                picks_str='All',\n",
    "                                shuffle_labels=shuf_labs,\n",
    "                                batch_size=batch_size,\n",
    "                                n_cv_folds=n_cv_folds,\n",
    "                                smooth_winsize=smooth_winsize,\n",
    "                                temp_gen=False,\n",
    "                                save_single_rep_scores=True,\n",
    "                                save_patterns=True,\n",
    "                                save_scores=True)\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "mne"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
