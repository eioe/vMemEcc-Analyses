{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0419f731",
   "metadata": {},
   "source": [
    "# Data Preparation: setting up the data for further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf9c758",
   "metadata": {},
   "source": [
    "Prepares EEG data for further processing steps & extract events. \n",
    "\n",
    "- Calculate bipolar EOG channels\n",
    "- Set channel type of ECG channel\n",
    "- extract events (in a robust way, fixing some minor issues that happened during a few recordings)\n",
    "\n",
    "If you want to work with clean data right away (recommended), skip this step and load the clean files from the `01_prepared` folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdbe971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "import mne\n",
    "from pathlib import Path\n",
    "from library import helpers, config\n",
    "\n",
    "\n",
    "def get_data_and_events(subID):\n",
    "    fname_inp = op.join(config.paths['00_raw'], subID + '-raw.fif')\n",
    "    raw = mne.io.read_raw_fif(fname_inp)\n",
    "    events, event_id = mne.events_from_annotations(raw)        \n",
    "    return raw, events, event_id\n",
    "\n",
    "def save_events(subID, events, event_id, bad_epos, epo_part):\n",
    "    fname_eve = op.join(config.paths['01_prepared-events'], '-'.join([subID, epo_part,'eve.fif']))\n",
    "    mne.write_events(fname_eve, events)\n",
    "    fname_eve_id = op.join(config.paths['01_prepared-events'], '-'.join([subID, 'event_id.pkl']))\n",
    "    if event_id is not None:\n",
    "        with open(fname_eve_id, 'wb') as f:\n",
    "            pickle.dump(event_id, f)\n",
    "    fname_bad_epos = op.join(config.paths['01_prepared-events'], '-'.join([subID, 'bad_epos_recording.pkl']))\n",
    "    with open(fname_bad_epos, 'wb') as f:\n",
    "        pickle.dump(bad_epos, f)\n",
    "    return\n",
    "\n",
    "def calc_eog_chans(data_raw):\n",
    "    # calculate and add HEOG and VEOG channels:\n",
    "\n",
    "    # if IO1 is not present, the old labels shall be used\n",
    "    if not 'IO1' in data_raw.ch_names:\n",
    "        rn_ch_dict = {\n",
    "            'AF7': 'IO1',\n",
    "            'AF8': 'IO2', \n",
    "            'FT9': 'LO1',\n",
    "            'FT10': 'LO2' \n",
    "        }\n",
    "        data_raw.rename_channels(rn_ch_dict) \n",
    "        print('renaming eog channels.')\n",
    "\n",
    "    ## which labels were given to EOG electrodes:\n",
    "    # 'vertical_chans' : \n",
    "    #        'left': ['Fp1', 'IO1'], \n",
    "    #        'right': ['Fp2', 'IO2'] \n",
    "    # 'horizontal_chans' : \n",
    "    #        'left': ['LO1'], \n",
    "    #        'right': ['LO2']\n",
    "\n",
    "\n",
    "    # For a few subjects electrodes 'Fp1' and 'IO1' were mistakenly exchanged. \n",
    "    # Let's find out for which and repair it:\n",
    "\n",
    "    picks = ['Fp1', 'Fp2', 'IO1', 'IO2', 'LO1', 'LO2']\n",
    "    rr = data_raw.load_data().copy().pick_channels(picks).filter(l_freq = 1, h_freq= 5, picks=['eeg','misc'], verbose=False)\n",
    "    # Create pseudo epochs to loop over:\n",
    "    events = mne.make_fixed_length_events(rr, duration=20)\n",
    "    epochs = mne.Epochs(rr, events, tmin=0.0, tmax=20, baseline = (0,1), verbose= False)\n",
    "    epochs.load_data().reorder_channels(picks)\n",
    "    # For each epoch we calculate the correlations between all EOG channels and store it\n",
    "    holder = [] \n",
    "    for epo in epochs:\n",
    "        epo = epo - epo.copy().mean(axis = 1, keepdims=True)\n",
    "        tmp = np.corrcoef(epo)\n",
    "        holder.append(tmp)\n",
    "\n",
    "    res = np.stack(holder).mean(axis=0)\n",
    "    \n",
    "    # correlations of \"Fp1\" (what could be \"IO1\")\n",
    "    idx_fp1 = epochs.ch_names.index('Fp1')\n",
    "    corrs_fp1 = res[idx_fp1,:]\n",
    "    # corr with itself is ofc largest, so we put it away\n",
    "    corrs_fp1[idx_fp1] = -999\n",
    "    # now get which other chan it correlates most with\n",
    "    idx_corrmax = np.argmax(corrs_fp1)\n",
    "    chan_corrmax = epochs.ch_names[idx_corrmax]\n",
    "    helpers.print_msg(f'What we think is Fp1 actually correlates highly with {chan_corrmax} (r = {str(np.max(corrs_fp1))})).')\n",
    "    \n",
    "    if not chan_corrmax == 'Fp2':\n",
    "        helpers.print_msg('Swopping channels IO1 and Fp1.')\n",
    "        tmp = data_raw.get_data(picks = ['Fp1', 'IO1'])\n",
    "        data_raw['Fp1'] = tmp[1]\n",
    "        data_raw['IO1'] = tmp[0]\n",
    "\n",
    "\n",
    "    # calculate bipolar EOG chans:\n",
    "    data_raw.load_data()\n",
    "    dataL = data_raw.get_data(['Fp1']) - data_raw.get_data(['IO1']) \n",
    "    dataR = data_raw.get_data(['Fp2']) - data_raw.get_data(['IO2']) \n",
    "    dataVEOG = np.stack((dataL,dataR), axis=0).mean(0)\n",
    "    \n",
    "    dataHEOG = data_raw.get_data(['LO1']) - data_raw.get_data(['LO2']) \n",
    "    dataEOG = np.concatenate((dataVEOG, dataHEOG), axis=0)\n",
    "    info = mne.create_info(ch_names=['VEOG', 'HEOG'], sfreq=raw.info['sfreq'], ch_types=['eog', 'eog'])\n",
    "    rawEOG = mne.io.RawArray(dataEOG, info=info)\n",
    "    data_raw.add_channels([rawEOG], force_update_info=True)\n",
    "    # set chan type of original channels to EEG:\n",
    "    ch_type_dict = {\n",
    "        'IO1': 'misc',\n",
    "        'IO2': 'misc', \n",
    "        'LO1': 'misc', \n",
    "        'LO2': 'misc'\n",
    "    }\n",
    "    data_raw.set_channel_types(ch_type_dict)\n",
    "\n",
    "    return data_raw\n",
    "\n",
    "def set_ecg_chan(data_raw):\n",
    "    ch_type_dict = {\n",
    "        'ECG': 'ecg'\n",
    "    }\n",
    "    data_raw.set_channel_types(ch_type_dict)\n",
    "\n",
    "def load_data_raw(filename, path):\n",
    "    ff = op.join(path, filename + '.fif')\n",
    "    return mne.io.Raw(ff)\n",
    "    \n",
    "\n",
    "def setup_event_structures(events_, event_id_, srate_):\n",
    "\n",
    "    # Define relevant events:\n",
    "    targ_evs_orig = [i for i in np.arange(150, 174)]\n",
    "    targ_evs = [event_id_['Stimulus/S%03d' % ss] for ss in targ_evs_orig]\n",
    "\n",
    "    epo_keys = ['CueL', 'CueR', 'LoadLow', 'LoadHigh', 'EccS', 'EccM', 'EccL']\n",
    "\n",
    "    event_dict = {key: [] for key in epo_keys}\n",
    "    for ev in targ_evs:\n",
    "        ev0 = ev - 150\n",
    "        if (ev0 % 2) == 0:\n",
    "            event_dict['CueL'].append(str(ev))\n",
    "        else:\n",
    "            event_dict['CueR'].append(str(ev))\n",
    "\n",
    "        if (ev0 % 8) < 4:\n",
    "            event_dict['LoadLow'].append(str(ev))\n",
    "        else:\n",
    "            event_dict['LoadHigh'].append(str(ev))\n",
    "        \n",
    "        if (ev0 % 24) < 8:\n",
    "            event_dict['EccS'].append(str(ev))\n",
    "        elif (ev0 % 24) > 15:\n",
    "            event_dict['EccL'].append(str(ev))\n",
    "        else:\n",
    "            event_dict['EccM'].append(str(ev))\n",
    "\n",
    "    \n",
    "    # clean from double markers in restarted trials:\n",
    "    # same routine as in ET analysis\n",
    "\n",
    "    # temporary list of trial onsets (fixation Onsets) and StimOnsets:\n",
    "    tmp_ev_fix = np.array([ev for ev in events_ if ev[2] in targ_evs])\n",
    "    stimon_times = np.array([ev[0] for ev in events_ if ev[2] == event_id_['Stimulus/S  2']])\n",
    "    for i in range(1,len(tmp_ev_fix)):\n",
    "        # We know that for restarted trials the same trial type comes 2x in a row:\n",
    "        if tmp_ev_fix[i,2] == tmp_ev_fix[i-1,2]: \n",
    "            times_between = np.arange(tmp_ev_fix[i-1,0], tmp_ev_fix[i,0])\n",
    "            # if no StimOnset between these two markers:\n",
    "            if not len(np.intersect1d(times_between, stimon_times)) > 0:\n",
    "                # we know that this was a restarted trial, and we overwrite all events between the two \n",
    "                # relevant markers:\n",
    "                idx = np.in1d(events_[:,0], times_between)\n",
    "                for ee in events_[idx]:\n",
    "                    ev_name = [n for n,v in event_id_.items() if v == ee[2]]\n",
    "                    print('Trial restarted: Overwriting event -- ' + str(ev_name))\n",
    "                events_[idx,2] = 999\n",
    "\n",
    "    # crop off all markers before first and after last exp block:\n",
    "    # TODO: refactor this\n",
    "    key_b1_start = 'Stimulus/S208'\n",
    "    if (key_b1_start in event_id_):\n",
    "        trig_b1_start = event_id_[key_b1_start]\n",
    "    else:\n",
    "        trig_b1_start = events[0][2] #use first event ever in case of doubt\n",
    "        print(\"Warning: No event START BLOCK01 found. Using first event in structure.\")\n",
    "    # same for end of last block:\n",
    "    key_b10_end = 'Stimulus/S247'\n",
    "    if (key_b10_end in event_id_):\n",
    "        trig_b10_end = event_id_[key_b10_end]\n",
    "    else:\n",
    "        trig_b10_end = events[-1][2]\n",
    "        print(\"Warning: No event END BLOCK10 found. Using last event in structure.\")\n",
    "    rel_evs = events_[:,2]\n",
    "    idx_start = np.where(rel_evs == trig_b1_start)[0][0] #use first element\n",
    "    idx_end = np.where(rel_evs == trig_b10_end)[0][-1] #use last element to get last instance\n",
    "    events_ = events_[idx_start:idx_end+1,:]\n",
    "\n",
    "    # set up event arrays:\n",
    "    events_fix_ = np.array([ev for ev in events_ if ev[2] in targ_evs])\n",
    "    # add duration of blinking interval:\n",
    "    events_fix_[:,0] = events_fix_[:,0] + config.times_dict['blink_dur'] * srate_\n",
    "\n",
    "    events_cue_ = np.array([ev for ev in events_ if ev[2] == event_id_['Stimulus/S  1']])\n",
    "    events_stimon_ = np.array([ev for ev in events_ if ev[2] == event_id_['Stimulus/S  2']])\n",
    "\n",
    "    bad_epos = dict()\n",
    "    # Check if for all relevant events 720 instances were found:\n",
    "    if any(np.array([len(events_fix_), len(events_cue_), len(events_stimon_)]) < 720):\n",
    "        \n",
    "        # Missing triggers that code for the trial type are tricky. It's better to fix that manually instead of silently tring to fix it.\n",
    "        if (len(events_fix_) != 720): \n",
    "            raise ValueError(\"There is a trigger missing that codes for the trial type. You have to fix this manually, I'm afraid!\")\n",
    "        \n",
    "        # For CueOnset triggers we can check their distance to the trial onsets (ie, fixation onset triggers). \n",
    "        # If no stimonset trigger is found in the relevant time window, this is probably the trial where something went wrong.  \n",
    "        if (len(events_cue_) != 720): \n",
    "            idx = np.argwhere([np.min(np.abs(ev[0] - events_cue_[:,0])) >             \n",
    "                (config.times_dict['fix_dur'] + 0.1) * srate for ev in events_fix_])\n",
    "            if (len(idx) != (720 - len(events_cue_))): \n",
    "                raise ValueError(\"There is a timing issue apart from missing triggers. Check your triggers manually!\")\n",
    "            else: \n",
    "                for i in idx: \n",
    "                    print('Replacing missing trigger for Cue and adding to \"bads\" list.')\n",
    "                    new_ev = [int(events_fix_[i[0],0] + config.times_dict['fix_dur'] * srate_), 0, event_id_['Stimulus/S  1']]\n",
    "                    events_tmp = np.vstack([events_cue_, new_ev])\n",
    "                    # sort events ascending in time:\n",
    "                    events_cue_ = events_tmp[events_tmp[:,0].argsort()]\n",
    "            bad_epos['cue'] = idx.flatten()\n",
    "\n",
    "\n",
    "        # For StimOnset triggers we can check their distance to the trial onsets (ie, fixation onset triggers). \n",
    "        # If no stimonset trigger is found in the relevant time window, this is probably the trial where something went wrong.  \n",
    "        if (len(events_stimon_) != 720): \n",
    "            idx = np.argwhere([np.min(np.abs(ev[0] - events_stimon_[:,0])) >             \n",
    "            (config.times_dict['fix_dur'] + config.times_dict['cue_dur'] + 0.1) * srate for ev in events_fix_])\n",
    "            if (len(idx) != (720 - len(events_stimon_))): \n",
    "                raise ValueError(\"There is a timing issue apart from missing triggers. Check your triggers manually!\")\n",
    "            else: \n",
    "                for i in idx: \n",
    "                    print('Replacing missing trigger for StimOnset and adding to \"bads\" list.')\n",
    "                    new_ev = [int(events_fix_[i[0],0] + (config.times_dict['fix_dur'] + config.times_dict['cue_dur']) * srate_), 0, event_id_['Stimulus/S  2']]\n",
    "                    events_tmp = np.vstack([events_stimon_, new_ev])\n",
    "                    # sort events ascending in time:\n",
    "                    events_stimon_ = events_tmp[events_tmp[:,0].argsort()]\n",
    "            bad_epos['stimon'] = idx.flatten()\n",
    "    \n",
    "    # Make sure that trials are in the same order (sorted by time)\n",
    "    events_fix_ = events_fix_[events_fix_[:,0].argsort()]\n",
    "    events_stimon_ = events_stimon_[events_stimon_[:,0].argsort()]\n",
    "    events_cue_ = events_cue_[events_cue_[:,0].argsort()]\n",
    "    assert len(events_fix_) == len(events_stimon_) == len(events_cue_)\n",
    "    \n",
    "    # Add trial type info to last column:\n",
    "    events_stimon_[:,2] = events_fix_[:,2]\n",
    "    events_cue_[:,2] = events_fix_[:,2]\n",
    "\n",
    "    return events_fix_, events_cue_, events_stimon_, bad_epos\n",
    "\n",
    "#FIXME: event_id\n",
    "def extract_epochs_ICA(raw_data, events, event_id_, n_jobs = 1):\n",
    "    # filter the data:\n",
    "    filtered = raw_data.load_data().filter(l_freq=1, h_freq=40, n_jobs = n_jobs)\n",
    "    epos_ica_ = mne.Epochs(filtered, \n",
    "                        events, \n",
    "                        event_id=event_id_, \n",
    "                        tmin=-0.6, \n",
    "                        tmax=2.3, \n",
    "                        baseline=(None,None),\n",
    "                        preload=False)\n",
    "    return epos_ica_\n",
    "\n",
    "#FIXME: event_id\n",
    "def extract_epochs_stimon(raw_data, events, event_id_, bad_epos_, n_jobs = 1):\n",
    "    # filter the data:\n",
    "    filtered = raw_data.load_data().filter(l_freq=0.01, h_freq=40, n_jobs = n_jobs)\n",
    "    epos_stimon_ = mne.Epochs(filtered, \n",
    "                        events, \n",
    "                        event_id=event_id_, \n",
    "                        tmin=-0.6, \n",
    "                        tmax=2.3, \n",
    "                        baseline=None,\n",
    "                        preload=False)\n",
    "    epos_stimon_.drop(bad_epos_, 'BADRECORDING')\n",
    "    return epos_stimon_\n",
    "\n",
    "def extract_epochs_cue(raw_data, events, event_id_, tmin_, tmax_, bad_epos_, n_jobs = 1):\n",
    "    # filter the data:\n",
    "    filtered = raw_data.load_data().filter(l_freq=0.01, h_freq=40, n_jobs = n_jobs)\n",
    "    epos_cue_ = mne.Epochs(filtered, \n",
    "                        events, \n",
    "                        event_id=event_id_, \n",
    "                        tmin=tmin_, \n",
    "                        tmax=tmax_, \n",
    "                        baseline=None,\n",
    "                        preload=False)\n",
    "    epos_cue_.drop(bad_epos_, 'BADRECORDING')\n",
    "    return epos_cue_\n",
    "\n",
    "def extract_epochs_fulllength(raw_data, events, event_id_, tmin_, tmax_, bad_epos_, n_jobs = 1):\n",
    "    # filter the data:\n",
    "    filtered = raw_data.load_data().filter(l_freq=0.01, h_freq=40, n_jobs = n_jobs)\n",
    "    epos_cue_ = mne.Epochs(filtered, \n",
    "                        events, \n",
    "                        event_id=event_id_, \n",
    "                        tmin=tmin_, \n",
    "                        tmax=tmax_, \n",
    "                        baseline=None,\n",
    "                        preload=False)\n",
    "    epos_cue_.drop(bad_epos_, 'BADRECORDING')\n",
    "    return epos_cue_\n",
    "\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "## Full procedure:\n",
    "sub_list = np.setdiff1d(np.arange(1,config.n_subjects_total+1), config.ids_missing_subjects)\n",
    "sub_list_str = ['VME_S%02d' % sub for sub in sub_list]\n",
    "\n",
    "## to run a single subject, modify and uncomment the following line:\n",
    "# sub_list_str = ['VME_S01']\n",
    "\n",
    "#sub_list = np.array([sub_list_str[job_nr]])\n",
    "\n",
    "for idx, subID in enumerate(sub_list_str):\n",
    "    helpers.print_msg('Processing subject ' + subID + '.')\n",
    "    \n",
    "    # Get data:\n",
    "    raw, events, event_id = get_data_and_events(subID)\n",
    "    \n",
    "    # Calculate EOG channels & set chan type:\n",
    "    raw = calc_eog_chans(raw)   \n",
    "    \n",
    "    # Set ECG chan type:\n",
    "    set_ecg_chan(raw)\n",
    "    \n",
    "    # Save prepared data:\n",
    "    helpers.save_data(raw,\n",
    "                      subID + '-prepared',\n",
    "                      config.paths['01_prepared'],\n",
    "                      append='-raw') \n",
    "\n",
    "    print(\"***Saving events:***\")\n",
    "    # Extract and save events:\n",
    "    srate = raw.info['sfreq']\n",
    "    events_fix, events_cue, events_stimon, bad_epos = setup_event_structures(events, event_id, srate)\n",
    "    save_events(subID, events_stimon, event_id=event_id, bad_epos=bad_epos, epo_part='stimon')\n",
    "    save_events(subID, events_cue, event_id=None, bad_epos=bad_epos, epo_part='cue') # enough to save event_id once\n",
    "    save_events(subID, events_fix, event_id=None, bad_epos=bad_epos, epo_part='fix')\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "mne"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
